{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('/Users/saikrishna/Downloads/cleaned_data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    usernames = [row[0] for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "# Replace 'GITHUB_TOKEN' with your actual GitHub token\n",
    "g = Github('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = []\n",
    "\n",
    "for username in usernames:\n",
    "    try:\n",
    "        user = g.get_user(username)\n",
    "        repos = user.get_repos()\n",
    "        for repo in repos:\n",
    "            contents = repo.get_contents(\"\")\n",
    "            while contents:\n",
    "                file_content = contents.pop(0)\n",
    "                if file_content.type == 'dir':\n",
    "                    contents.extend(repo.get_contents(file_content.path))\n",
    "                else:\n",
    "                    # Process code files (e.g., .py, .js, .java)\n",
    "                    if is_code_file(file_content.name):\n",
    "                        code = file_content.decoded_content.decode('utf-8')\n",
    "                        # Analyze the code using Cohere API\n",
    "                        analysis = analyze_code_with_cohere(code)\n",
    "                        user_data.append({\n",
    "                            'username': username,\n",
    "                            'language': detect_language(file_content.name),\n",
    "                            'complexity': analysis['complexity'],\n",
    "                            'syntax_quality': analysis['syntax_quality']\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user {username}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client('COHERE_API')\n",
    "\n",
    "def analyze_code_with_cohere(code):\n",
    "    # Use Cohere's generation or embedding models to analyze code\n",
    "    response = co.generate(\n",
    "        model='command',\n",
    "        prompt=f\"Analyze the following code for complexity and syntax quality:\\n\\n{code}\\n\\nProvide metrics for complexity and syntax quality.\",\n",
    "        max_tokens=50,\n",
    "        temperature=0.5,\n",
    "        stop_sequences=[\"\\n\"]\n",
    "    )\n",
    "    # Parse the response to extract metrics\n",
    "    return parse_cohere_response(response.generations[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def is_code_file(filename):\n",
    "    code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.cs', '.rb', '.go']\n",
    "    return os.path.splitext(filename)[1] in code_extensions\n",
    "\n",
    "def detect_language(filename):\n",
    "    return os.path.splitext(filename)[1][1:]  # Returns extension without dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cohere_response(response_text):\n",
    "    # Implement parsing logic based on the response format\n",
    "    # For example, extract numerical values from the text\n",
    "    complexity = extract_complexity(response_text)\n",
    "    syntax_quality = extract_syntax_quality(response_text)\n",
    "    return {\n",
    "        'complexity': complexity,\n",
    "        'syntax_quality': syntax_quality\n",
    "    }\n",
    "\n",
    "def extract_complexity(text):\n",
    "    # Implement regex or string parsing to find complexity score\n",
    "    pass\n",
    "\n",
    "def extract_syntax_quality(text):\n",
    "    # Implement regex or string parsing to find syntax quality score\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame(user_data)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df[['complexity_norm', 'syntax_quality_norm']] = scaler.fit_transform(df[['complexity', 'syntax_quality']])\n",
    "df['total_points'] = df['complexity_norm'] + df['syntax_quality_norm']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
